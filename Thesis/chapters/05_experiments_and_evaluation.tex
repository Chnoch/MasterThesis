\newpage
\section{Experiments and Evaluation}
\label{sec:experiments}

The main goal of our thesis was to compare different state of the art algorithms and to combine machine learning techniques to gain a deeper understanding of what is necessary and what is possible in order to achieve the best results based on an initial data set and certain evaluation targets that one wants to find. In this chapter we explore our procedure from the initial dataset that we work with to the final comparison of the results that we produced.



\subsection{Introduction / Procedure}

At first we did an initial data analysis and created a naive approach that as a way to gain further insight into our data as a whole as well as to create a proof-of-concept whether the assumptions we had about our data are justifiable. 
We then continue to use actual machine learning algorithms implemented into the WEKA toolkit that is created and maintained as an open source project by the University of New Zealand. As part of this we discuss our execution plan, the different features that we want to combine, execute and evaluate and the way we will gather the results and compare them.

In order to have data that the algorithms can use directly and to avoid outliers or false cases we combine statistical analysis together with data preparation to convert our raw dataset into a reasonably filtered and correctly formatted set. As discussed in Section~\ref{subsec:proposed_solution} we used a Decision Tree, a Naive Bayes algorithm and a Hidden Markov Model to train and evaluate our dataset. After improving the process and gathering the results we compare the different execution plans and algorithms and analyze the impact of different design decisions of our approach.

\subsection{Data Set / Data Analysis}
\label{subsec:data_set}
The first step to getting started with machine learning is to know about the data and the underlying domain associated with it. Machine learning is not an oracle where all the data can be inserted and it spits out everything you ever wondered about. Careful analysis and a knowledge of the domain of the data are inexcusable. 

\subsubsection{Data Set}
Our data was gathered over months from active user of an Android smartphone app. The app (Farplano) gives the user an overview of the current timetable of train and bus stations close to him. The timetable is based on an open data set from SBB (Swiss Railway company). The app contains features such as automatic geolocation, full route information of bus and train lines, arbitrary connection planning and many more. In addition to that it also allowed the user (with an opt-in feature) to track his position and anonymously store the stations with timestamps that he passes. We worked on this gathered dataset from hundreds of users over many months. The raw data thus contains an entry for every station that the user passes. A single data entry contains the following data points:
\begin{itemize}
	\item Anonymized User ID (a 9-character String)
	\item Station Id (a 7-character String)
	\item Timestamp
\end{itemize}
The information contained in the raw dataset is thus relatively trivial. In order to be able to successfully predict the future location of a user we needed to combine multiple entries to get further information. As a first step we split up the data set to separate sets for each user. We also sorted the entries by timestamp, so that we have a chronological view of all the events as they actually happened.

Remark: We have purposefully left out global state and information about usage patterns, mostly due to the added complexity as well as performance reasons. However this might be something that could be analyzed and included in Future Work. 

\subsubsection{Data Analysis}
To get a basic understanding of what kind of data that we have and to be able to reason about our data set we created a number of charts, based on the features that we will later use for the machine learning algorithms. 

As a first analysis we looked at how the data is distributed by time and day, as can be seen in the following two charts.

\begin{figure}[!ht]
	\caption{Distribution by Time of Day}
	\centering
	\includegraphics[width=1.0\textwidth]{charts/distribution_time_of_day}
\end{figure}

\begin{figure}[!ht]
	\caption{Distribution by Day of Month}
	\centering
	\includegraphics[width=1.0\textwidth]{charts/day_of_month_distribution}
\end{figure}

As we expected we encountered the highest usage during commuting hours, especially in the afternoon. The distribution gets significantly lower as midnight is approached. During the night we have very few gathered data points. The only somewhat surprising point was that the commute hours in the morning didn't produce as high a peak as the hours in the afternoon. However the fact that the data from the app is gathered voluntarily and the app needs to be open to collect data might explain these small inconsistencies.

The distribution by day of month didn't really produce significant insight. The fact that in the end of the month the number of stops are significantly lower is due to fewer months having 31 days. The data set isn't normalized against this and will therefore include such issues. However what this tells us is that the day of month might not be a good indicator. It would be worth exploring the difference between weekday and weekend-day. Since commuting behavior is generally vastly different on weekends as on weekdays this comparison might lead to deeper and more succinct insights.

\begin{figure}[!ht]
	\caption{Number of Stations per User}
	\centering
	\includegraphics[width=1.0\textwidth]{charts/different_stations_per_user}
\end{figure}


\begin{figure}[!ht]
	\caption{Number of Stops per User}
	\centering
	\includegraphics[width=1.0\textwidth]{charts/stops_per_user}
\end{figure}

\begin{figure}[!ht]
	\caption{Number of Users per Station}
	\centering
	\includegraphics[width=1.0\textwidth]{charts/persons_per_station}
\end{figure}

A vastly more interesting and also challenging conclusion could be drawn from comparing the users with the stations they frequent and how often they stop at stations. As we expected there are a few users that have amassed a lot of data and then there is a long tail of less frequent users. The same statistic also applies to the number of stations and the number of stops that have been gathered for a user. What we already realized here is that it will be very difficult if not impossible to get a good prediction for the users tailing the statistics, simply because there is just not enough data available. In some edge cases where a user really only has 2 or 3 stops that he regularly frequents this might work, otherwise it will just be stabbing in the dark to get a reasonably good prediction. One or two outliers from such a user could possibly mix up the complete prediction process. It seems sensible to cut the dataset into high- and low-frequency users and discard the latter. The exact boundary or whether it will be flexible to a certain degree will have to be tested by trial and error, however if we would not cut the low-frequency user out of our comparison tests it might greatly change our conclusions and the effectiveness of our process.

\begin{figure}[!ht]
	\caption{Distribution of Stops per Station}
	\centering
	\includegraphics[width=1.0\textwidth]{charts/distribution_stops_by_station}
\end{figure}

\begin{figure}[!ht]
	\caption{Average number of accesses per station by users}
	\centering
	\includegraphics[width=1.0\textwidth]{charts/number_stations_average_user}
\end{figure}

A similar conclusion can also be drawn by the more averaging figures that we created. It also shows a relatively small set with a lot of data and a long, small tail. Combined with the previous conclusions this strengthened our approach of doing statistical analysis and preparing the data set to remove data points that will skew our result. The types of preparation, analysis and restrictions we've imposed on our data set are described in detail in ~\ref{subsubsec:data_preparation}

\subsection{Naive Approach}

As a first approach we wanted to create a reasonable starting point to work with our data set. For this we created a naive approach in how we first measured and analyzed our data. 

We took our complete data set without doing any statistical analysis or restriction and created a graph for each user. The graph consists of nodes and edges. Each node represent a station and we drew an edge from a node to another node if the destination was the immediate successor of a station in our data set. This created a graph that looked similar to ----Chart below---- and that reflects how the users travel across the network of stations. Our guess was that since a user might generally take similar paths on his travels this would be reflected in the graph and could give us a good first estimate.

For the analysis we then looked at the node that was to be analyzed (assuming this is the node the user would currently be at) and just took the path with the most outgoing edges as our prediction. We then validated this against the ground truth that we had established previously, where we just ordered all the stops at the stations in chronological order. The expected prediction would therefore always be the station that appears next in chronological order for a user. 

This naive approach with all its known limitations and issues gave us a correct prediction of 50 percent (----- double check -----). We have purposefully chosen not to limit our data set or to put it under certain limitations. This obviously leads to a worse overall prediction and also to some wrong predictions that will be classified as correct, since our ground truth does not necessarily fully reflect the actual reality. However our goal was to start out with a reasonable estimate on the basis of our data set. We didn`t want to prematurely optimize issues away that wouldn`t really disturb our prediction models. With that approach we could start out from the very base of our prediction and then analyze the effects that statistical optimizations and a better preparation of our data set have. Thanks to the naive approach that we created we could make sure that what we analyzed with machine learning techniques and the statistical analysis and preparations that we did were actual optimizations and gains in precision.

\subsubsection{Web GUI, Visual Representation}

\subsection{Machine Learning Preparation}
\label{subsec:machine_learning_results}
As outlined before applying machine learning techniques to our problem was the main goal of our thesis. After having gathered insights into our dataset by statistical means and by looking at the dataset with a naive approach we started working on machine learning. We laid out our execution plan, defining what and how we want to test the different algorithms on our dataset, what kind of algorithms we'd use and how we evaluate and compare the results. The following sections are the result of carrying out these plans.

\subsubsection{Execution Plan}
The reason to have a proper execution plan is to have a reproducible way of evaluating the features with different classifiers and reason about the gathered results. 

The classifiers that we use are already described in a theoretical manner in section \ref{subsec:proposed_solution}. We have used a Decision Tree (J48, unpruned ------ WEKA Link --------) and a Naive Bayes classifier both provided from WEKA. As a classifier with back propagation we have used a Multilayer Perceptron classifier also provided by WEKA.

To test the accuracy of each of the classifiers trained with different features we are using the F1 score. The F1 score considers both the precision and the recall. In information retrieval the precision is defined as "How many selected items are relevant?" whereas the recall is defined as "How many relevant items are selected?". The combination of both precision and recall gives quite a good approximation of the accuracy of a classifier and allows us to easily compare the different results with each other. Mathematically the F1 score is defined as 
\begin{equation}\label{equation:F1Score}
F_{1} = 2 \cdot \frac{precision \cdot recall}{precision + recall}
\end{equation}

The F1 score is also called traditional F-measure or balanced F-score, because it's the harmonic mean of precision and recall and thus weighs both measurements equally.

We are using the precision, recall and F1 score directly to compare the different executions and to analyze their results. However we also split up the result set to compare the precision/recall/F1 for the number of most frequent stations used as well as for different types of users (frequent/non-frequent). With that analysis we should be able to see how the prediction for different usage patterns and types of users are influenced by our classifiers.

The features that we have been able to identify from our data set and that we have worked with are the following:
\begin{itemize}
	\item Current Station
	\item Day of Week
	\item Hour of Day
	\item Minute of Hour
	\item Weekday or Weekendday
	\item Previous Station
	\item \textbf{Next station} (to be predicted)
\end{itemize}

The current station can directly be deduced from the gathered data. The same can be done thanks to the timestamp for the day of week, hour of day, minute of hour and the weekday/weekendday flag. For the previous station we had to do some calculation as well as some assumptions about the data. The way we have prepared the previous station is described in more detail in section \ref{subsubsec:data_preparation}.  

The next station is the feature that we will be predicting with our algorithms. It is also called our ground truth. We know that our ground truth does not necessarily correspond to the actual reality, but it is the closest assumption we can make with the data and the resources we have at hand. Improving the ground truth would require either the actual implementation of the prediction into the Android app and letting the user give feedback on the prediction or sifting through the data manually and analyzing whether our ground truth makes sense given the user's behavior. Unfortunately both options are out of scope for this thesis, leading us to rely on our somewhat unreliable ground truth. The issue that could arise from that is that we optimize for a scenario that does not properly reflect reality and that would leave our results useless or require a lot of improvements for an actual real world use case.

In our execution plans we have used different combinations of the feature set above to train and then test the data set. We have always tried to predict the next station feature.


\subsubsection{Data Preparation / Statistical Analysis}
\label{subsubsec:data_preparation}
As part of the process to convert the raw data we have gathered into a WEKA-compatible format we also implemented the findings we gathered from our data analysis, from the naive approach and from the subsequent execution of our plans and the gathering of the results. We constantly improved the results of our predictions by taking the data with the preparation we had done so far, using the machine learning algorithms on the data and then analyzing the results. This process has not been a straightforward path where we knew what and how we needed to implement restrictions and enhancements on our data, but much more a continuous path, leading to small or significant improvements or decreases in precision. Based on the analysis of each step we took we were able to get a good precision, albeit at the cost described in this section. We outline all the transformations, restrictions and enhancements that we made on our data set here.

\paragraphtitle{Data Parsing} 
The first step in the process was to parse our data set. The data is available as a single large .csv files, each entry consisting of a line such as this:
\begin{lstlisting}
2015-11-26T23:25:46	path=/v1/training?stop_id=8592010	hashacc=595604002
\end{lstlisting}
Each line starts with the timestamp of the entry, followed by the api path that was made, the id of the station as well as a hash of the user. The relevant data for us is obviously the timestamp, the user hash (which is subsequently directly used as user id) and the station id. The api path is the same for every entry and is irrelevant for our use case. Directly during the import of the data into our model we separated each entry by user and discarded the entries that had an empty user id attached. There is always a timestamp and a station id available. We have also during the import already added the time-based additional fields to our model. These fields include the hour of day, minute of hour, the day of week and whether it's a weekday or weekend day. We also directly set the station id. The previous and next station are added in a later step. A model entry is internally represented by the following fields:
\begin{lstlisting}
public class ModelEntry {
	private String userId
	
	private Date timestampStart
	
	private int hourOfDay
	private int minuteOfHour
	private int dayOfWeek
	private Boolean weekday
	
	private String stationId
	private String previousStationId
	
	// This is the main thing to predict
	private String nextStationId
}
\end{lstlisting}
Now that we have completed the import of the raw data into our model we have a representation of every user with every entry that was gathered from said user. 

\paragraphtitle{Remove Duplicates}
The next step is to remove duplicate entries that are present in a user's entry list. Because the Android app responsible for gathering the data and sending it to the server periodically pulls the current location. If a user has to wait at a station for a few minutes it's possible that the app created not only one but multiple entries for this station. In our prediction later we don't want to predict how many times the app would gather data from the same station, but want to predict the actually target destination of the user. That's the reasoning behind removing the duplicate entries. Recognition and removal of duplicates works as follows: We sort all the entries of a user by its timestamp and then look at two consecutive entries. If the time difference of the two entries lies within a certain threshold, we discard the second entry and only keep the first. After some tests we have come to the conclusion that 60 seconds is a reasonable threshold for discarding duplicate entries. Since the app does periodically update its location data we can be relatively sure that within 60 seconds we catch all the cases that are actually just duplicate entries. In case a user has two consecutive entries with e.g. 2 hours time difference, it's more likely that the user is actually starting a new journey at the second entry. Therefore it wouldn't make sense to discard the second entry, especially since each entry is also tied to the specific time and day it was recorded and that information is then also used in our prediction algorithms.

\paragraphtitle{Assign Previous / Next Stations}
Once all duplicates are removed from the entries we assign the previous and next station to each entry. As can be seen later in the experiments the previous station is quite beneficial in contributing to overall precision. As a general rule we assigned the station id of the previous data entry of the user as the previous station. However we are doing a split at night at 04.00. At this time a new day in our dataset begins and if the previous data entry was part of the last day we simply set the previous station id of the current entry to "null". This limitation is done in order to not distort the different days too much. We use the same algorithm to set the next station id.

\paragraphtitle{Remove Low Profile Users}
Once both the previous and next station have been assigned to an entry we analyze the data for each user to remove all the low profile users. We have realized that users which have barely used the app and gathered very limited data are very hard to predict. We have set the threshold to the amount of data entries that a user needs to have to 20. It has given us a good compromise to not exclude too many users and in the process only working with very few samples while still maintaining a high precision and good quality of our predictions. It might be an option to fine tune this value, taking into account for example how concise a users data is. For a user that only ever goes to three different stations we might be able to successfully predict his future movements earlier while it is a lot harder to generate e reasonable model for a user that gathers data only sporadically and in lots of different places. However for the data set we have at hand the threshold we chose is a good compromise.

\paragraphtitle{Analyze Station Usage}
The next preparation step we take analyzes the usages of the stations for each user. The filtering we do based on this statistical analysis has led to the highest gain in prediction quality, mostly because we have filtered out a lot of noisy data and additional low profile users that generally pull down the overall precision. We first count the number of occurrences of the most frequently used stations of a user. If those most often used stations still have an occurrence count lower than a certain threshold we discard the user. The threshold in our experiments has been set at 10 occurrences, which has shown to be a reasonable trade-off. 
What we also do is limit the number of stations that we want to predict. It is generally unreasonable to try and predict every possible station that the user could be based on his previous interactions or even based on the full data set. It has also shown that predictions are quite unstable when trying to predict every single possible station given the data that we have. We have therefore limited the number of stations for our prediction feature to ten stations per user. This number has also been gathered after some tests have shown that it gives a good overall prediction. In that step we therefore also remove the next station id from each entry should it not be within the ten stations that are to be predicted. In this case we add the value 'null' as the next station, signaling that we can't make a prediction based the data available in that entry. The value 'null' is also added to the values that the machine learning toolkit can actually predict. In that case it would show the system that none of the most prominent stations could be predicted. If WEKA predicts a 'null' value for a data entry and the ground truth actually does contain 'null' we also count that as a successful prediction, because after all the system also needs to know when it can't yet properly predict something. Obviously with more data and more processing power we might be able to push the number of possible stations higher, but for our evaluations we have left the threshold constant.

After all those operations we have successfully prepared our raw data into a reasonable set that we can be comfortable handing off to WEKA to build the models from our data, evaluate, cross-validate and display the results. This process as well as the results are discussed in the following section.

\subsection{Machine Learning Results}
Before we present the results we want to outline the way we pass data to WEKA and the computation steps that WEKA internally does.

The data that we can pass to WEKA need to be represented by Attributes. Each feature that we defined is matched by its representative attribute. The instances of the data that are passed to WEKA should all contain allowed values for the attributes. An attribute can have one of the following types (from \cite{WEKAJavaDocAttribute}):
\begin{itemize}
	\item numeric: represents a number (floating-point)
	\item nominal: represents a fixed set of values
	\item string: represents a dynamic set of values
	\item date: represents a date
	\item relational: represents a relation between multiple instances
\end{itemize}


We have only used nominal attributes since the data set we had always consisted of a value from a fixed set. Since we haven't used the full date directly but have split it up into day, hour, minute, etc. we have also used nominal values for these. The set of values for current, previous and next station for a user is every value that each of the features consisted of (including 'null' for previous and next station).

For each entry available for a user we have created an Instance. A WEKA Instance consist of a name (the user id in our case), every possible attribute and the actual value for each attribute. Each instance also can also define a class index that will be used to represent the class for prediction purposes. We have always set the class index to the attribute that represents our next station feature.

The instances are passed to a classifier from WEKA. A classifier is the actual algorithm that builds the appropriate model for our instances. There are many classifiers that are provided by WEKA directly, such as J48 (a decision-tree based implementation), NaiveBayes or MultilayerPerceptron. There are also many open source libraries that extend the Classifier interface from WEKA that can be directly integrated into the WEKA toolkit. Classifiers can then build the model from the instances that are passed. It's also possible to incrementally create or update the model of a classifier. Since we only did offline training and testing this was not needed in our case. However should our system ever be used with real world data we would most certainly need to use incremental model building as we would not want to recreate our full model each time our data set updates.

Once the model is created we use an Evaluation to evaluate the model. There are different ways to train and test a model. One way is to generate a separate training and testing set (usually splitting the original data set by 2 to 1). Another way is to use (k-fold) cross-validation. In cross validation, the original data set is split into equally big subsets. One of those subsets is retained for testing, he other k-1 subsets are used for training. Usually this process is repeated multiple times to erase possible issues with the randomness of the subset generation. We have used 10 times 10-fold cross-validation (where enough data was present) to train and test our instances with our classifiers.

Once the classifiers has been trained and tested, WEKA provides an abundance of information about its predictions. We used this information to put together charts and statistical results for each of our classifier given each of the different feature sets we prepared for it.

\subsection{Experiments}
We have conducted three different experiments on our data set, each with different algorithms and with multiple feature sets. The algorithms are always Decision Tree, Naive Bayes and Multilayer Perceptron. On the last experiment we have omitted the Multilayer Perceptron algorithm due to severe performance issues.

For each experiment and each algorithm we have used three different feature sets to train and test the algorithm:

\mbox{}\\
\begin{tabular}{l | c | r}
	\hline
	\textbf{Stations} & \textbf{Current Station \& Time-Based} & \textbf{All Features} \\
	\hline
	 Current Station  & Current Station & Current Station \\
	 Previous Station & Hour of Day & Hour of Day \\
	  & Minute of Hour & Minute of Hour \\
	  & Day of Week & Day of Week \\
	  & Weekday & Weekday \\
	  & & Previous Station \\
	  \hline
\end{tabular}



\newpage
\subsubsection{Comparison of all Users}
For our first experiment we take our fully prepared data set, run each algorithm with each feature set once per user and collect the results of the evaluation. We then sort the results of the users by their F-Values for the line chart in order to get a good representation of how precision and recall correlates and the F-Measure deteriorates in our data set. The Boxplot graph gives a good statistical overview of the distribution and the highs and lows in our result set. As it is generally helpful to have multiple representations of data we have in most cases included both the line chart and boxplot graph.

\paragraphtitle{Decision Trees}
% Decision Tree - Stations
\begin{figure}[H]
	\makebox[\textwidth]{
		\begin{minipage}[t]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/decision_tree_stas_line}
		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/decision_tree_stas_boxplot}
		\end{minipage}
		\hfill
	}
	\caption{Stations Features for Decision Tree}
\end{figure}

% Decision Tree - Current Station & Timestamp
\begin{figure}[H]
	\makebox[\textwidth]{
		\begin{minipage}[t]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/decision_tree_csta_ts_line}
		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/decision_tree_csta_ts_boxplot}
		\end{minipage}
		\hfill
	}
	\caption{Current Station and Time-Based Features for Decision Tree}
\end{figure}

% Decision Tree - All Features
\begin{figure}[H]
	\makebox[\textwidth]{
		\begin{minipage}[t]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/decision_tree_all_line}
		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/decision_tree_all_boxplot}
		\end{minipage}
		\hfill
	}
	\caption{All Features for Decision Tree}
\end{figure}


\paragraphtitle{Naive Bayes}
% Naive Bayes - Stations
\begin{figure}[H]
	\makebox[\textwidth]{
		\begin{minipage}[t]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/naive_bayes_stas_line}
		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/naive_bayes_stas_boxplot}
		\end{minipage}
		\hfill
	}
	\caption{Stations Features for Naive Bayes}
\end{figure}

% Naive Bayes  - Current Station & Timestamp
\begin{figure}[H]
	\makebox[\textwidth]{
		\begin{minipage}[t]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/naive_bayes_csta_ts_line}
		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/naive_bayes_csta_ts_boxplot}
		\end{minipage}
		\hfill
	}
	\caption{Current Station and Time-Based Features for Naive Bayes}
\end{figure}

% Naive Bayes  - All Features
\begin{figure}[H]
	\makebox[\textwidth]{
		\begin{minipage}[t]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/naive_bayes_all_line}
		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/naive_bayes_all_boxplot}
		\end{minipage}
		\hfill
	}
	\caption{All Features for Naive Bayes}
\end{figure}


\paragraphtitle{Multilayer Perceptron}
% Multilayer Perceptron - Stations
\begin{figure}[H]
	\makebox[\textwidth]{
		\begin{minipage}[t]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/mp_stas_line}
		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/mp_stas_boxplot}
		\end{minipage}
		\hfill
	}
	\caption{Stations Features for Multilayer Perceptron}
\end{figure}

% Multilayer Perceptron - Current Station & Timestamp
\begin{figure}[H]
	\makebox[\textwidth]{
		\begin{minipage}[t]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/mp_csta_ts_line}
		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/mp_csta_ts_boxplot}
		\end{minipage}
		\hfill
	}
	\caption{Current Station and Time-Based Features for Multilayer Perceptron}
\end{figure}

% Multilayer Perceptron - All Features
\begin{figure}[H]
	\makebox[\textwidth]{
		\begin{minipage}[t]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/mp_all_line}
		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined/mp_all_boxplot}
		\end{minipage}
		\hfill
	}
	\caption{All Features for Multilayer Perceptron}
\end{figure}


\newpage
\subsubsection{Comparison of frequent versus non-frequent Users}
In this experiment we have let the algorithm run through our data set as before, however before creating the graph for our evaluation we have split the result set into high- and low-frequency users. The split is done by 50\% of our data set. We simply count the number of data entries a user has to see whether that user belongs into the high- or low-frequency belt. In the graphs below the left-hand side of the graph represent the high-frequency users and the right-hand side the low-frequency users. Each half is then ordered by their respective F-Values just as before.

\paragraphtitle{Decision Trees}
% Decision Tree - Stations
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{files/high_vs_low_frequency/decision_tree_stas}
	\caption{Stations Features for Decision Tree}
\end{figure}

% Decision Tree - Current Station & Timestamp
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{files/high_vs_low_frequency/decision_tree_csta_ts}
	\caption{Current Station and Time-Based Features for Decision Tree}
\end{figure}

% Decision Tree - All Features
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{files/high_vs_low_frequency/decision_tree_all}
	\caption{All Features for Decision Tree}
\end{figure}


\paragraphtitle{Naive Bayes}
% Naive Bayes - Stations
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{files/high_vs_low_frequency/naive_bayes_stas}
	\caption{Stations Features for Naive Bayes}
\end{figure}

% Naive Bayes - Current Station & Timestamp
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{files/high_vs_low_frequency/naive_bayes_csta_ts}
	\caption{Current Station and Time-Based Features for Naive Bayes}
\end{figure}

% Naive Bayes - All Features
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{files/high_vs_low_frequency/naive_bayes_all}
	\caption{All Features for Naive Bayes}
\end{figure}


\paragraphtitle{Multilayer Perceptron}
% Multilayer Perceptron - Stations
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{files/high_vs_low_frequency/mp_stas}
	\caption{Stations Features for Multilayer Perceptron}
\end{figure}

% Multilayer Perceptron - Current Station & Timestamp
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{files/high_vs_low_frequency/mp_csta_ts}
	\caption{Current Station and Time-Based Features for Multilayer Perceptron}
\end{figure}

% Multilayer Perceptron - All Features
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{files/high_vs_low_frequency/mp_all}
	\caption{All Features for Multilayer Perceptron}
\end{figure}


\newpage
\subsubsection{Comparison of all Users with higher number of Stations}
\paragraphtitle{Decision Trees}
% Decision Tree - Stations
\begin{figure}[H]
	\makebox[\textwidth]{
		\begin{minipage}[t]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined_more_station/decision_tree_stas_line}
		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined_more_station/decision_tree_stas_boxplot}
		\end{minipage}
		\hfill
	}
	\caption{Stations Features for Decision Tree}
\end{figure}

% Decision Tree - Current Station & Timestamp
\begin{figure}[H]
	\makebox[\textwidth]{
		\begin{minipage}[t]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined_more_station/decision_tree_csta_ts_line}
		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined_more_station/decision_tree_csta_ts_boxplot}
		\end{minipage}
		\hfill
	}
	\caption{Current Station and Time-Based Features for Decision Tree}
\end{figure}

% Decision Tree - All Features
\begin{figure}[H]
	\makebox[\textwidth]{
		\begin{minipage}[t]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined_more_station/decision_tree_all_line}
		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined_more_station/decision_tree_all_boxplot}
		\end{minipage}
		\hfill
	}
	\caption{All Features for Decision Tree}
\end{figure}


\paragraphtitle{Naive Bayes}
% Naive Bayes - Stations
\begin{figure}[H]
	\makebox[\textwidth]{
		\begin{minipage}[t]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined_more_station/naive_bayes_stas_line}
		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined_more_station/naive_bayes_stas_boxplot}
		\end{minipage}
		\hfill
	}
	\caption{Stations Features for Naive Bayes}
\end{figure}

% Naive Bayes  - Current Station & Timestamp
\begin{figure}[H]
	\makebox[\textwidth]{
		\begin{minipage}[t]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined_more_station/naive_bayes_csta_ts_line}
		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined_more_station/naive_bayes_csta_ts_boxplot}
		\end{minipage}
		\hfill
	}
	\caption{Current Station and Time-Based Features for Naive Bayes}
\end{figure}

% Naive Bayes  - All Features
\begin{figure}[H]
	\makebox[\textwidth]{
		\begin{minipage}[t]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined_more_station/naive_bayes_all_line}
		\end{minipage}
		\hfill
		\begin{minipage}[t]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{files/all_users_combined_more_station/naive_bayes_all_boxplot}
		\end{minipage}
		\hfill
	}
	\caption{All Features for Naive Bayes}
\end{figure}





\subsection{Comparison of Results}
